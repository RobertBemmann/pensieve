---
title: "CV Robert Bemmann"
permalink: /cv/
layout: posts
author_profile: true
---

# CV Robert Bemmann

_Data Engineer based in Berlin ðŸ‡©ðŸ‡ª_ <br>

[Email](mailto:bemmann.data@gmail.com) / [Website](https://robertbemmann.github.io/pensieve/) / [LinkedIn](https://www.linkedin.com/in/robert-bemmann/)

## :man_technologist: Engineering Experience

**Senior Data Engineer** @ [Getyourguide](https://www.getyourguide.com/) _(Apr 2020 - Present)_ <br>
Berlin-based online travel agency and online marketplace for tour guides and excursions.
  - Led the design and development of mirroring our production Databases (15, >300 tables, core ETL) via Debezium Change Data Capture streaming technology (DB->Kafka->S3->Spark->DWH), owned project and roadmap end-to-end, cutting landing times by improving the SLA for 1 hour, cutting costs by factor 50, built with Kafka, Spark, Airflow
  - Migrated our self-service BI Tool Looker from Databricks (Spark) to Snowflake, incl. for automated tests via Looker API (shared in [public webinar w/ Snowflake](https://resources.snowflake.com/customer-stories/getyourguide-turning-travel-dreams-into-reality-with-snowflake), [blog article](https://medium.com/tech-getyourguide/migrating-our-self-service-bi-tool-looker-from-hive-apache-spark-to-snowflake-492441bca934) and Berlin data meetup)
  - Redesigned and centralized the architecture and layout of our daily prio1 reports end-to-end via Spark, Snowflake, Airflow and Looker
  - Build an application which enables the monitoring of our orchestration tool Airflow for tasks statistics (landing times) and its dependencies, reducing root cause analysis to seconds
  - Held internal learning session on Spark performance optimizations and shared learning on improvements on how to test our existing self service Spark SQL processing framework
  - Responsible for initial design and event ingestion for our CRM data model (event enrichment from Braze source data as well as building CRM subscriptions from existing events)
  - Building ETLs via Spark (S3 data lake) and enhancing data models (various teams like Finance, CRM, etc.)
  - Main technologies/tools: Apache Spark, Snowflake, Apache Airflow, Looker, Kafka, Terraform
  - Main languages: Python, Scala, SQL

<br><br>

## Skills & Aptitudes

## Education

## Internships